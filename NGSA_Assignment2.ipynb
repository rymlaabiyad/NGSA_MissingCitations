{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import igraph\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "from sklearn.metrics import make_scorer, accuracy_score \n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit, train_test_split\n",
    "import csv\n",
    "import nltk\n",
    "\n",
    "perc = 0.0625\n",
    "#Possible values for the 'model' variable are : 'knn', 'bagging', 'svm', 'logreg'\n",
    "model = 'knn'\n",
    "pred_file = model+\"_improved_predictions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bagging_improved_predictions.csv'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "1001 training examples processsed\n",
      "2001 training examples processsed\n",
      "3001 training examples processsed\n",
      "4001 training examples processsed\n",
      "5001 training examples processsed\n",
      "6001 training examples processsed\n",
      "7001 training examples processsed\n",
      "8001 training examples processsed\n",
      "9001 training examples processsed\n",
      "10001 training examples processsed\n",
      "11001 training examples processsed\n",
      "12001 training examples processsed\n",
      "13001 training examples processsed\n",
      "14001 training examples processsed\n",
      "15001 training examples processsed\n",
      "16001 training examples processsed\n",
      "17001 training examples processsed\n",
      "18001 training examples processsed\n",
      "19001 training examples processsed\n",
      "20001 training examples processsed\n",
      "21001 training examples processsed\n",
      "22001 training examples processsed\n",
      "23001 training examples processsed\n",
      "24001 training examples processsed\n",
      "25001 training examples processsed\n",
      "26001 training examples processsed\n",
      "27001 training examples processsed\n",
      "28001 training examples processsed\n",
      "29001 training examples processsed\n",
      "30001 training examples processsed\n",
      "31001 training examples processsed\n",
      "32001 training examples processsed\n",
      "33001 training examples processsed\n",
      "34001 training examples processsed\n",
      "35001 training examples processsed\n",
      "36001 training examples processsed\n",
      "37001 training examples processsed\n",
      "38001 training examples processsed\n",
      "1 testing examples processsed\n",
      "1001 testing examples processsed\n",
      "2001 testing examples processsed\n",
      "3001 testing examples processsed\n",
      "4001 testing examples processsed\n",
      "5001 testing examples processsed\n",
      "6001 testing examples processsed\n",
      "7001 testing examples processsed\n",
      "8001 testing examples processsed\n",
      "9001 testing examples processsed\n",
      "10001 testing examples processsed\n",
      "11001 testing examples processsed\n",
      "12001 testing examples processsed\n",
      "13001 testing examples processsed\n",
      "14001 testing examples processsed\n",
      "15001 testing examples processsed\n",
      "16001 testing examples processsed\n",
      "17001 testing examples processsed\n",
      "18001 testing examples processsed\n",
      "19001 testing examples processsed\n",
      "20001 testing examples processsed\n",
      "21001 testing examples processsed\n",
      "22001 testing examples processsed\n",
      "23001 testing examples processsed\n",
      "24001 testing examples processsed\n",
      "25001 testing examples processsed\n",
      "26001 testing examples processsed\n",
      "27001 testing examples processsed\n",
      "28001 testing examples processsed\n",
      "29001 testing examples processsed\n",
      "30001 testing examples processsed\n",
      "31001 testing examples processsed\n",
      "32001 testing examples processsed\n"
     ]
    }
   ],
   "source": [
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "with open(\"testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]\n",
    "        \n",
    "# note: Kaggle requires that you add \"ID\" and \"category\" column headers\n",
    "\n",
    "###############################\n",
    "# beating the random baseline #\n",
    "###############################\n",
    "\n",
    "# the following script gets an F1 score of approximately 0.66\n",
    "\n",
    "# data loading and preprocessing \n",
    "\n",
    "# the columns of the data frame below are: \n",
    "# (1) paper unique ID (integer)\n",
    "# (2) publication year (integer)\n",
    "# (3) paper title (string)\n",
    "# (4) authors (strings separated by ,)\n",
    "# (5) name of journal (optional) (string)\n",
    "# (6) abstract (string) - lowercased, free of punctuation except intra-word dashes\n",
    "\n",
    "with open(\"training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "\n",
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [element[0] for element in node_info]\n",
    "\n",
    "# compute TFIDF vector of each paper\n",
    "corpus = [element[5] for element in node_info]\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "# each row is a node in the order of node_info\n",
    "features_TFIDF = vectorizer.fit_transform(corpus)\n",
    "\n",
    "## the following shows how to construct a graph with igraph\n",
    "## even though in this baseline we don't use it\n",
    "## look at http://igraph.org/python/doc/igraph.Graph-class.html for feature ideas\n",
    "\n",
    "edges = [(element[0],element[1]) for element in training_set if element[2]==\"1\"]\n",
    "\n",
    "## some nodes may not be connected to any other node\n",
    "## hence the need to create the nodes of the graph from node_info.csv,\n",
    "## not just from the edge list\n",
    "\n",
    "nodes = IDs\n",
    "\n",
    "## create empty directed graph\n",
    "g = igraph.Graph(directed=True)\n",
    " \n",
    "## add vertices\n",
    "g.add_vertices(nodes)\n",
    " \n",
    "## add edges\n",
    "g.add_edges(edges)\n",
    "\n",
    "## Find the giant connected component\n",
    "gcc = []\n",
    "for idx, v in enumerate(g.components().giant().vs):\n",
    "    gcc.append(v['name'])\n",
    "    \n",
    "#Google PageRank\n",
    "page_rank = g.pagerank()\n",
    "\n",
    "# for each training example we need to compute features\n",
    "# in this baseline we will train the model on only 5% of the training set\n",
    "\n",
    "# randomly select 5% of training set\n",
    "to_keep = random.sample(range(len(training_set)), k=int(round(len(training_set)*perc)))\n",
    "training_set_reduced = [training_set[i] for i in to_keep]\n",
    "\n",
    "# we will use three basic features:\n",
    "\n",
    "# number of overlapping words in title\n",
    "overlap_title = []\n",
    "\n",
    "# temporal distance between the papers\n",
    "temp_diff = []\n",
    "\n",
    "# number of common authors\n",
    "comm_auth = []\n",
    "\n",
    "# Common journal\n",
    "journal = [element[4] for element in node_info]\n",
    "comm_journal = np.zeros(len(training_set_reduced))\n",
    "\n",
    "# TD-IDF cosine similarity\n",
    "cos_similarity = []\n",
    "\n",
    "#Graph related features\n",
    "jaccard = []\n",
    "jaccard_in = []\n",
    "jaccard_out = []\n",
    "degrees = []\n",
    "in_gcc = np.zeros(len(training_set_reduced))\n",
    "shortest_path = []\n",
    "page_rank_diff = []\n",
    "\n",
    "counter = 0\n",
    "for i in range(len(training_set_reduced)):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "\t# convert to lowercase and tokenize\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "\t# remove stopwords\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    \n",
    "    overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "    \n",
    "    if(journal[index_source] == journal[index_target] and journal[index_source] != ''):\n",
    "        comm_journal[i] = 1\n",
    "    cos_similarity.append(cosine_similarity(features_TFIDF.getrow(index_source),\n",
    "                                    features_TFIDF.getrow(index_target))[0][0])\n",
    "    \n",
    "    jaccard.append(g.similarity_jaccard(pairs = [(index_source, index_target)],\n",
    "                                              loops = False)[0])\n",
    "    jaccard_in.append(g.similarity_jaccard(pairs = [(index_source, index_target)],\n",
    "                                              mode = 'IN', loops = False)[0])\n",
    "    jaccard_out.append(g.similarity_jaccard(pairs = [(index_source, index_target)],\n",
    "                                              mode = 'OUT', loops = False)[0])\n",
    "    \n",
    "    degrees.append(g.strength(index_source)*g.strength(index_target))\n",
    "    #if(source_info[0] in gcc or target_info[0] in gcc):\n",
    "        #in_gcc[i] = 1\n",
    "        \n",
    "    u = g.shortest_paths_dijkstra(source=index_source, target=index_target, mode=3)[0][0]\n",
    "    if u >1500000:\n",
    "        shortest_path.append(-1)\n",
    "    else:\n",
    "        shortest_path.append(u)\n",
    "        \n",
    "    page_rank_diff.append(page_rank[index_target] - page_rank[index_source])\n",
    "    \n",
    "   \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print (counter, \"training examples processsed\")\n",
    "\n",
    "# convert list of lists into array\n",
    "# documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "training_features = np.array([overlap_title, temp_diff, comm_auth, \n",
    "                              comm_journal, cos_similarity,\n",
    "                              jaccard, jaccard_in, jaccard_out, \n",
    "                              degrees, shortest_path, page_rank_diff]).T\n",
    "\n",
    "# scale\n",
    "training_features = preprocessing.scale(training_features)\n",
    "\n",
    "# convert labels into integers then into column array\n",
    "labels = [int(element[2]) for element in training_set_reduced]\n",
    "labels = list(labels)\n",
    "labels_array = np.array(labels)\n",
    "\n",
    "\n",
    "# test\n",
    "# we need to compute the features for the testing set\n",
    "\n",
    "overlap_title_test = []\n",
    "temp_diff_test = []\n",
    "comm_auth_test = []\n",
    "comm_journal_test = np.zeros(len(testing_set))\n",
    "cos_similarity_test = []\n",
    "#Graph related features\n",
    "jaccard_test = []\n",
    "jaccard_in_test = []\n",
    "jaccard_out_test = []\n",
    "degrees_test = []\n",
    "in_gcc_test = np.zeros(len(testing_set))\n",
    "shortest_path_test = []\n",
    "page_rank_diff_test = []\n",
    "   \n",
    "counter = 0\n",
    "for i in range(len(testing_set)):\n",
    "    source = testing_set[i][0]\n",
    "    target = testing_set[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    \n",
    "    overlap_title_test.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff_test.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth_test.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "    \n",
    "    if(journal[index_source] == journal[index_target] and journal[index_source] != ''):\n",
    "        comm_journal_test[i] = 1\n",
    "    cos_similarity_test.append(cosine_similarity(features_TFIDF.getrow(index_source), \n",
    "                                            features_TFIDF.getrow(index_target))[0][0])\n",
    "    \n",
    "    jaccard_test.append(g.similarity_jaccard(pairs = [(index_source, index_target)], \n",
    "                                              loops=False)[0])\n",
    "    jaccard_in_test.append(g.similarity_jaccard(pairs = [(index_source, index_target)], \n",
    "                                              mode = 'IN', loops=False)[0])\n",
    "    jaccard_out_test.append(g.similarity_jaccard(pairs = [(index_source, index_target)], \n",
    "                                              mode = 'OUT', loops=False)[0])\n",
    "    \n",
    "    degrees_test.append(g.strength(index_source)*g.strength(index_target))\n",
    "    #if(source_info[0] in gcc or target_info[0] in gcc):\n",
    "        #in_gcc_test[i] = 1\n",
    "    \n",
    "    u = g.shortest_paths_dijkstra(source=index_source, target=index_target, mode=3)[0][0]\n",
    "    if u >1500000:\n",
    "        shortest_path_test.append(-1)\n",
    "    else:\n",
    "        shortest_path_test.append(u)\n",
    "   \n",
    "    page_rank_diff_test.append(page_rank[index_target] - page_rank[index_source])\n",
    "    \n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print (counter, \"testing examples processsed\")\n",
    "        \n",
    "# convert list of lists into array\n",
    "# documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "testing_features = np.array([overlap_title_test, temp_diff_test, comm_auth_test, \n",
    "                             comm_journal_test, cos_similarity_test,\n",
    "                            jaccard_test, jaccard_in_test, jaccard_out_test,\n",
    "                            degrees_test, shortest_path_test, page_rank_diff_test]).T\n",
    "\n",
    "# scale\n",
    "testing_features = preprocessing.scale(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Splitting the reduced data into training and dev sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(training_features, labels_array, \n",
    "                                                  test_size=0.2, random_state=10)\n",
    "\n",
    "#Creating cross validation data splits\n",
    "cv_sets = StratifiedShuffleSplit(n_splits = 2, test_size = 0.20, random_state = 5)\n",
    "cv_sets.get_n_splits(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score for bagging  :  0.999805042890564\n",
      "Testing score for bagging  :  0.9897322589030413\n"
     ]
    }
   ],
   "source": [
    "# initialize classifier\n",
    "def train_model(model):\n",
    "    if(model == 'svm'):\n",
    "        classifier = svm.SVC()\n",
    "        parameters = {\"kernel\": [\"rbf\", \"poly\"], 'C': [1, 10, 20, 50, 100], 'gamma' : [1e-3, 1e-4]}\n",
    "        return classifier, parameters\n",
    "    elif(model == 'logreg'):\n",
    "        classifier = LogisticRegression()\n",
    "        parameters = {\"multi_class\": [\"multinomial\"], \"penalty\": [\"l2\"], \n",
    "            'C': [1, 10, 50, 100], \"solver\": [\"newton-cg\", \"lbfgs\"], \"max_iter\": [300]}\n",
    "        return classifier, parameters\n",
    "    elif(model == 'bagging'):\n",
    "        classifier = BaggingClassifier(random_state=10)\n",
    "        parameters = {'n_estimators' : [10, 20, 50, 100], 'max_features' : [1, 2, 3]}\n",
    "        return classifier, parameters\n",
    "    elif(model == 'knn'):\n",
    "        classifier = KNeighborsClassifier()\n",
    "        parameters = {\"n_neighbors\": [3, 5, 7], \"weights\": [\"uniform\", \"distance\"],\n",
    "                      \"algorithm\": [\"ball_tree\", \"kd_tree\"], \"leaf_size\": [20, 30, 40]}\n",
    "        return classifier, parameters\n",
    "    else:\n",
    "        print('Please choose a correct classifier')\n",
    "        return\n",
    "\n",
    "# train\n",
    "classifier, parameters = train_model(model)\n",
    "best_classifier = GridSearchCV(classifier, parameters, \n",
    "                            scoring = make_scorer(accuracy_score), cv = cv_sets)\n",
    "best_classifier.fit(x_train, y_train)\n",
    "\n",
    "#Compute accuracy scores\n",
    "acc_train = accuracy_score(y_train, best_classifier.predict(x_train))\n",
    "acc_test = accuracy_score(y_test, best_classifier.predict(x_test))\n",
    "print('Training score for', model, ' : ', str(acc_train))\n",
    "print('Testing score for', model, ' : ', str(acc_test))\n",
    "\n",
    "# issue predictions\n",
    "predictions = list(best_classifier.predict(testing_features))\n",
    "\n",
    "# write predictions to .csv file suitable for Kaggle (just make sure to add the column names)\n",
    "predictions = zip(range(len(testing_set)), predictions)\n",
    "\n",
    "headers = ['id', 'category']\n",
    "with open(pred_file,\"w\") as pred1:\n",
    "    csv_out = csv.writer(pred1)\n",
    "    csv_out.writerow(i for i in headers)\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
